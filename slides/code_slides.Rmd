---
title: "Meta-analysis in Comparative Physiology: A brief introduction to effect sizes and meta-analytic modelling"
author: Daniel W.A Noble, Nicholis Wu, Essie Rodgers, Patrice Pottier
output: powerpoint_presentation
date: '`r Sys.Date()`'
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr, magick)
```

## Slide with Bullets {.secondslide}

-   Bullet 1
-   Bullet 2
-   Bullet 3

$$
y_{i} = x^2
$$

> -   Eat eggs
> -   Drink coffee

## Slide with R Output {.thirdslide}

```{r cars, echo = TRUE, tidy=TRUE}
# LDH activity in 8 C treatment
 trt_8C <- rnorm(10, 0.40, 0.5)

# LDH activity in 26 C treatment
trt_26C <- rnorm(10, 0.60, 0.20)

# What's the difference, i.e., effect size, between treatments?
mean(trt_26C - trt_8C)
t.test(trt_26C, trt_8C)
```

## Do 1000s of experiments

```{r sim, echo = TRUE, tidy=TRUE}

experiment <- function(times){
  
  dat <- data.frame(matrix(nrow = times, ncol = 2))
  
  for(i in 1:times){
  # LDH activity in 8 C treatment
   trt_8C <- rnorm(10, 0.40, 0.5)
  
  # LDH activity in 26 C treatment
  trt_26C <- rnorm(10, 0.60, 0.20)
  
  # What's the difference, i.e., effect size, between treatments?
  dat[i,1] <- mean(trt_26C - trt_8C)
  dat[i,2] <- t.test(trt_26C, trt_8C)$p.value
    
  }
  names(dat) <- c("effect", "p")
  return(dat)
}

expts <- experiment(10000)
```

## Figure

```{r simfig, echo = FALSE, tidy=TRUE}
par(mfrow = c(1,2))
hist(expts$effect, xlab = "Effect size", main = "")
hist(expts$p, xlab = "p-value", main = "")
```

## errors

$$
SE = SD / \sqrt{N}
$$

```{r simfig2, echo = TRUE, tidy=TRUE}
sd(expts$effect)

```

## se

```{r simfig3, echo = TRUE, eval = FALSE, tidy=TRUE}
  means <- c()

   for (i in 1:1000000){
      mass <- rnorm(n = 18, mean = 10.8, sd = 0.988)
      means <- c(means, mean(mass))
    }

   # The mean of the means
   mean(means)
   # The standard deviation of the means or standard error
   sd(means)
```

## Our analytical calculaton

```{r se, echo = TRUE, eval = FALSE, tidy=TRUE}
se <- sd(mass) / sqrt(length(mass))
se
```

## Plots

```{r simfig3plot, echo = TRUE, eval = FALSE, tidy=TRUE}
   ggplot() + geom_histogram(aes(x = means))
```

## Zr

```{r Zr,  echo = TRUE}
zr_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/ind_disp_raw_data.csv") %>%
  dplyr::select(study_ID, taxa, species, trait, response, response_unit, disp_trait, disp_unit, corr_coeff, sample_size) %>% # remove irrelevant columns for this tutorial
  dplyr::top_n(10) # select first 10 effect sizes to illustrate escalc function
```

## Effect calculation

```{r Zrescalc,  echo = TRUE}
# Calculate Fisher's r-to-z transformed correlation coefficient (ZCOR) as yi = effect size and vi = sampling variances, where ri = raw correlation coefficients, and ni = sample size.
zr_data <- metafor::escalc(measure = "ZCOR", ri = corr_coeff, ni = sample_size, data = zr_data, var.names=c("Zr","v_Zr"))

zr_data

metafor::forest(x = zr_data$Zr[1:6], vi = zr_data$v_Zr[1:6], xlab = "Z-transformed Correlation (Zr)")
```

## Back transofrmation

```{r Zrescalcbackcalc, echo = TRUE}
# We can easily convert back to r as follows
zr_data$r <- tanh(zr_data$Zr)

zr_data %>% select(corr_coeff, r)
```

## Hedges

```{r lnRR, echo = TRUE}
contrast_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/pop_disp_raw_data.csv") %>%
  dplyr::select(study_ID, taxa, species, trait, response, response_unit, mean_core, sd_core, n_core, mean_front, sd_front, n_front) %>% # remove irrelevant columns for this tutorial
  dplyr::top_n(10) # select first 10 effect sizes to illustrate escalc function
```

## Hedgesg

```{r hedges, echo = TRUE}
# Calculate Hedges' g as g = effect size and v_g = sampling variances, where m1i = mean of edge population, n1i = sample size of edge population, sd1i = standard deviation of edge population, m2i = mean of core population, n2i = sample size of core population, sd2i = standard deviation of core population.
contrast_data <- metafor::escalc(measure = "SMD", 
                              m1i = mean_front, n1i = n_front, sd1i = sd_front,
                              m2i = mean_core, n2i = n_core, sd2i = sd_core, 
                              data = contrast_data, var.names=c("g","v_g"))
contrast_data %>% select(study_ID, mean_core, sd_core, n_core, mean_front, sd_front, n_front, g, v_g)
```

## lnRR

```{r lnRREffects, echo = TRUE, eval = FALSE}
# Calculate log response ratio, lnRR = effect size and v_lnRR = sampling variances, where m1i = mean of edge population, n1i = sample size of edge population, sd1i = standard deviation of edge population, m2i = mean of core population, n2i = sample size of core population, sd2i = standard deviation of core population.
contrast_data <- metafor::escalc(measure = "ROM", 
                              m1i = mean_front, n1i = n_front, sd1i = sd_front,
                              m2i = mean_core, n2i = n_core, sd2i = sd_core, 
                              data = contrast_data, var.names=c("lnRR","v_lnRR"))
contrast_data %>% select(study_ID, mean_core, sd_core, n_core, mean_front, sd_front, n_front, lnRR, v_lnRR)
#contrast_data$cluster <- c(1,1,1,2,2,3,4,5,6,7,8)
contrast_data$obs <- 1:nrow(contrast_data)
data <- contrast_data
covariance_funtions <- function(data, m, sd, n, cov_type){

  if(cov_type == "ROM"){
    # Covariance for shared control when using log response ratio. From Jageunesse 2011. Ecology
    cov <- data[, sd]^2 / (data[, n] * data[, m]^2)
  }

  if(cov_type == "LOR"){
    #formula for odds ratio (1/x + 1/(m-x))
    cov <- (1/data[, m] + 1) / (data[, n] - data[, m])
  }
  return(cov)
}

# Step 1: split on cluster
splt <- split(data, data$cluster)

# Step 2: calculate the covariance. Because cluster will have repeated values these will be the same within a study
data$cov <- unlist(lapply(splt, function(x) 
                  if(dim(x)[1] > 1) {
                    covariance_funtions(x, m = "mean_core",sd = "sd_core", n = "n_core", cov_type = "ROM") } else { 0}))

# Step 3: build matrix
m <- diag(data$v_lnRR)

m <- lapply(splt, function(x) matrix(x$v_lnRR))

m_splt <- split(m, data$cluster)
bdiag(m)
m[which(duplicated(data[,cluster])==TRUE), which(duplicated(data[,cluster])==TRUE)] <- data$cov[which(duplicated(data[,cluster])==TRUE)]

  m[upper.tri(m)] <- t(m[lower.tri(m)])

mat_splt <- lapply(splt, function(x) diag(x$v_lnRR))



mat_splt[[2]][lower.tri(mat_splt[[2]])] <- data$cov[1:3]


### Testing out new make_VCV with contrast_data as example
contrast_data$cluster <- c(1,1,2,2,3,3,3,0,0,0,0)
contrast_data$cluster <- paste0(contrast_data$cluster,".", contrast_data$study_ID)

metaAidR::make_VCV_matrix(data = contrast_data, V = "v_lnRR", obs = "obs", cluster = "cluster", type = "cor")

make_VCV_matrix(contrast_data, V = "v_lnRR", obs = "obs", cluster = "cluster", m = "mean_core", sd = "sd_core", n = "n_core", cov_type = "ROM")
```

## COnverting back to understand lnRR

```{r ConvertLnRR, echo = TRUE}
### Let's back calculate effects to make sure we understand why they are interpreted as percentage differences

# Make sure we understand how it's calculated
with(contrast_data, log(mean_front[1] / mean_core[1]))

# Alternatively....
with(contrast_data, log(mean_front[1]) - log(mean_core[1]))

# Now lets back-transform to odds. 
with(contrast_data, exp(log(mean_front[1]) - log(mean_core[1])))

# Interpretation: What this tells us is that the numerator is 1.15 times the denominator, or, that the marginal (front) mean is 15% larger compared to the core mean. We can see that this is true as follows:
with(contrast_data, exp(log(mean_front[1]) - log(mean_core[1])) * mean_core[1])

# This value should now match the marginal mean value, which it does
with(contrast_data, mean_front[1])
```

## Latency

$$
ln \bar{X} = log(\bar{X}) - log \sqrt{ \left ( 1 + \frac{SD^2}{\bar{X}^2}\right )}
$$

$$
lnSD = \sqrt{ log \left (   1 + \frac{SD^2}{\bar{X}^2} \right )}
$$

## Proportions

$$
\bar{X_{p}} = log \left ( \frac{p}{ 1 - p } \right ) 
$$

$$
SD_{p} = \sqrt{SD^2 \left ( \frac{1}{p} \right ) + \left ( \frac{p}{ 1 - p^2} \right )}
$$

## Arcsine transformation

$$
\bar{X_{t}} = arcsine(\sqrt{\bar{{X}}})
$$ $$
SD_{t} = \sqrt{\frac{SD^2}{4\bar{X}\left ( 1- \bar{X}\right ) }}
$$ \## Geary test - Counts

$$
\frac{\bar{X}}{SD} \left ( \frac{4N^{3/2}}{1 + 4N} \right ) \geq 3
$$

## Q10

$$
\begin{equation} 
  lnRR_{Q_{10}} = ln\left( \frac{R_{2}}{R_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
(\#eq:lnq10)
\end{equation} 
$$

$$
\begin{equation} 
  s^2_{lnRR_{Q_{10}}} = \left( \frac{SD_{2}^2}{R^2_{2}N_{2}} + \frac{SD_{1}^2}{R^2_{1}N_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }^2
  (\#eq:Vlnq10)
\end{equation} 
$$

## Fixed effect

```{r fixmodel, echo = TRUE}

# Generate some simulated effect size data with known sampling variance
# assumed to come from a common underlying distribution
set.seed(86) # Set see so that we all get the same simulated results
 # We will have 5 studies
    stdy  <- 1:5                                      
# We know the variance for each effect
    Ves     <- c(0.05, 0.10, 0.02, 0.10, 0.09)    
# We'll need this later but these are weights
    W     <- 1 / Ves                                          
# We assume they are sampled from a normal distribution  with a mean effect size of 2
    es     <- rnorm(length(Ves), 2, sqrt(Ves))          
# Data for our fixed effect meta-analysis
    dataFE <- data.frame(stdy = stdy, es, Ves)

```

## Fixed effects

```{r fePLOT, echo = FALSE}
  
  plot(stdy - 0.25 ~ es, col = "black", pch = 16, ylim = c(0, 6), xlim = c(1, 3.2), 
       data = dataFE, ylab = "Study", xlab = "Effect size")
  abline(v = 2, lty =3)
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es + sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es - sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
```

## Fixed effect model

```{r, fixedma,  eval=TRUE, echo=TRUE}
# Run a fixed effect meta-analysis using the FE dataset. 
    metafor::rma(yi = es, vi = Ves, method = "FE", data = dataFE)
```

## Now by hand FE

We can even do all the seemingly fancy stuff `metafor` is doing
ourselves if we want....we just need to know the equations: $$
  \bar{ES}= \sum{\left (W*ES \right )}/ \sum{W}
 $$ $$
  \sigma_{\bar{ES}}^2= \frac{1}{\sum{W}}
 $$

```{r, fixedmahand,  eval=TRUE, echo=TRUE}
# Calculate pooled effect size
       EsP.FE <- sum(W*dataFE$es) / sum(W)
       EsP.FE
# Calculate the pooled variance around estimate
    VarEsP.FE <- 1 / sum(W)
    VarEsP.FE
# Calculate the standard error around estimate
    SE.EsP.FE <- sqrt(VarEsP.FE)
    SE.EsP.FE
```

## Q10

$$
\begin{equation} 
  Q_{10} = \left( \frac{R_{2}}{R_{1}} \right)^{ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
\end{equation} 
$$
## lnq10
$$
\begin{equation} 
  lnRR_{Q_{10}} = ln\left( \frac{R_{2}}{R_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
\end{equation} 
$$
$$
\begin{equation} 
  s_{lnRR_{Q_{10}}} = \left( \frac{SD_{2}^2}{R^2_{2}N_{2}} + \frac{SD_{1}^2}{R^2_{1}N_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }^2
\end{equation} 
$$


## lnvrq10
$$
\begin{equation} 
lnVR_{Q_{10}} = ln\left( \frac{SD_{2}}{SD_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
\end{equation} 
$$
$$
\begin{equation} 
s_{lnVR_{Q_{10}}} = \left( \frac{1}{2\left( N_{2}-1 \right)} + \frac{1}{2\left(N_{1} - 1\right)} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }^2
\end{equation} 
$$

## lncvrq10
$$
\begin{equation} 
lnCVR_{Q_{10}} = ln\left( \frac{\text{CV}_{2}}{\text{CV}_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
\end{equation} 
$$

$$
\begin{equation} 
s_{lnCVR_{Q_{10}}} = \left[ \frac{(\text{SD}_{1})^2}{N_{1}({R}_{1})^2} + \frac{(\text{SD}_{2})^2}{N_{2} ({R}_{2})^2} + \frac{1}{2\left( N_{1}-1 \right)} + \frac{1}{2\left(N_{2} - 1\right)} \right]{ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }^2
\end{equation} 
$$

## Random effect in code

```{r randomSim , echo = TRUE}
# Here adding 0.8 says we want to add 0.8 as the between study variability. In other words, each effect size is sampled from a larger distribution of effect sizes that itself comes from a distribution with a variance of 0.8. 
    esRE        <- rnorm(length(Ves), 2, sqrt(Ves + 0.8)) 
# Data for our random effect meta-analysis 
    dataRE <-  data.frame(stdy = stdy, esRE, Ves)
```

## Random effect plot

```{r, fvsr, eval=TRUE, echo=FALSE, fig.cap="Mean (arrows are sampling standard deviation) effect size for each study. Data simulated under a fixed effect model in black and data simulated under a random effect model in red."}
    
  plot(stdy - 0.25 ~ es, col = "black", pch = 16, ylim = c(0, 6), xlim = c(1, 3.2), 
       data = dataFE, ylab = "Study", xlab = "Effect size")
  abline(v = 2, lty =3)
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es + sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es - sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  
  points(stdy + 0.25 ~ esRE, col = "red", pch = 16, data = dataRE)
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es + sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es - sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  
```

## random effect model

```{r, re,  eval=TRUE, echo=TRUE}
metafor::rma(yi = esRE, vi = Ves, method="DL", data = dataRE)
```

## Random by hand

We first need to estimate $\tau^2$ or the between-study variance which
can be calculated from these equations:

$$\tau^2 = \frac{Q - df}{C} $$ where

$$ Q = \sum_{i=1}^{k}{W_{i}ES_{i}^2} - \frac{ \left (\sum_{i=1}^{k}{W_{i}ES_{i}} \right)^2}{\sum_{i=1}^{k}{W_{i}}}$$
$$ C = \sum{W_{i}} - \frac{\sum{W_{i}^2}}{\sum{W_{i}}}$$

## Random effect by hand 2

```{r, re_hand, eval=TRUE, echo=TRUE}
# Calculate our Q statistic again
    Q <- sum(W*(dataRE$es^2) ) - (sum(W*dataRE$es)^2 / sum(W))
    Q

# Calculate tau2
    C <- sum(W) - ((sum(W^2))/sum(W))
    C

# Calculate df
    df <- nrow(dataRE) - 1   
    T2 <- (Q - df) / C
    T2
```

## Now that we have tau2 lets do the meta-analysis

Remember, things are the same but the weighting is different now.

```{r, re_w, eval=TRUE, echo=TRUE}
    # RE weights
      W.re <- 1 / (T2 + dataRE$Ves)
    
    #Pooled effect size for random effect meta-analysis
      esPoolRE  <- sum(W.re*dataRE$es) / sum(W.re) 
      esPoolRE
    
    # Calculate the pooled variance around estimate
       VarES <- 1 / sum(W.re)
    
    # Calculate the standard error around estimate
       SE.ES.RE <- sqrt(VarES)
       SE.ES.RE
```

## Q10 write our functioon

```{r ln10funcs, echo = TRUE, message=FALSE, warning=FALSE}

#' @title lnRR_Q10
#' @description Calculates the log Q10 response ratio.  Note that temperature 2 is placed in the numerator and temperature 1 is in the denominator
#' @param t1  Lowest of the two treatment temperatures
#' @param t2  Highest of the two treatment temperatures
#' @param r1  Mean physiological rate for temperature 1
#' @param r2  Mean physiological rate for temperature 2
#' @param sd1 Standard deviation for physiological rates at temperature 1
#' @param sd2 Standard deviation for physiological rates at temperature 2
#' @param n1  Sample size at temperature 1
#' @param n2  Sample size at temperature 2
#' @param name Character string for column name
#' @example
#' lnRR_Q10(20, 30, 10, 5, 1, 1, 30, 30)
#' lnRR_Q10(20, 30, 10, 5, 1, 1, 30, 30, name = "acclim")
#' @export

lnRR_Q10 <- function(t1, t2, r1, r2, sd1, sd2, n1, n2, name ="acute"){
        
    lnRR_Q10 <- (10 / (t2 - t1))   * log(r2 / r1)
  V_lnRR_Q10 <- (10 / (t2 - t1))^2 * ((sd1^2 / (n1*r1^2)) + (sd2^2 / (n2*r2^2)))
  
              dat <- data.frame(lnRR_Q10, V_lnRR_Q10)
    colnames(dat) <- c(paste0("lnRR_Q10", name),  
                       paste0("V_lnRR_Q10", name))
  return(dat)
}

```

## Q10

```{r ln101, echo = TRUE, message=FALSE, warning=FALSE}
q10_dat <- read.csv("https://osf.io/download/fb3ht/") 

# Calculate lnRRQ10 and it's associated sampling variance
q10_dat <- cbind(q10_dat, with(q10_dat, lnRR_Q10(t1 = t1, t2 = t2, r1 = mean_t1, r2 = mean_t2, sd1 = sd_t1, sd2 = sd_t2, n1 = n_t1, n2 = n_t2, name = "")))
head(q10_dat)

```

## Q10 Interpretation

```{r ln102, echo = TRUE,message=FALSE, warning=FALSE}
# Now we can back-calculate to put on the original Q10 scale. We can than check that this matches the Q10 already in the data
head(q10_dat %>% mutate(exp_lnRR_Q10 = exp(lnRR_Q10)) %>% select(exp_lnRR_Q10, Q10))

```

**For the first row of our data we can see that dive duration when
temperatures increase by 10**$^{\circ}$C is expected to decrease by
\~67% (1-0.33).

## MLMA

```{r, echo = TRUE}
# install.packages("pacman") ; uncomment this line if you haven't already installed 'pacman'
pacman::p_load(metafor, tidyverse)

asr_dat <- read.csv("https://osf.io/qn2af/download")
```

We'll also need our function for calculating ARR and its sampling
variance because these don't exist in any current packages.

```{r ARRfunc, echo = TRUE}
#' @title arr
#' @description Calculates the acclimation response ratio (ARR).  
#' @param t2_l  Lowest of the two treatment temperatures
#' @param t1_h  Highest of the two treatment temperatures
#' @param x1_h  Mean trait value at high temperature
#' @param x2_l  Mean trait value at low temperature
#' @param sd1_h Standard deviation of mean trait value at high temperature
#' @param sd2_l Standard deviation of mean trait value at low temperature
#' @param n1_h  Sample size at high temperature
#' @param n2_l  Sample size at low temperature

arr <- function(x1_h, x2_l, sd1_h, sd2_l, n1_h, n2_l, t1_h, t2_l){
        ARR <- (x1_h - x2_l)/(t1_h - t2_l)
      V_ARR <- ((1/(t1_h - t2_l))^2*(sd2_l^2/n2_l + sd1_h^2/n1_h))
return(data.frame(ARR, V_ARR))
}
```

## Caluclate ARR

```{r, tidy=TRUE, echo = TRUE}
# Calculate the effect sizes
asr_dat<-asr_dat %>% 
              mutate(ARR= arr(x1_h = mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                              sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,1], 
                     V_ARR = arr(x1_h =  mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                           sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,2]) %>% 
                filter(sex == "female")
```

## Fit model

```{r, class.source='klippy', echo = TRUE}
# Multi-level meta-analytic model
MLMA <- metafor::rma.mv(yi= ARR~ 1, V = V_ARR, 
                   method="REML",
                   random=list(~1|species_ID,
                               ~1|authors,
                               ~1|es_ID), 
                   dfs = "contain",
                   test="t",
                   data=asr_dat)
print(MLMA)

```

## Phylogeny1

```{r, phylo}
## Load packages
# install.packages("pacman")
pacman::p_load(tidyverse, metafor, ape, phytools)

zr_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/ind_disp_raw_data.csv") %>%
  dplyr::select(-dispersal_def) %>% # remove irrelevant columns
  tibble::rowid_to_column("es_ID") %>% # add effect size ID
  dplyr::mutate(species_OTL = stringr::str_replace_all(species_OTL, "_", " ")) # remove underscore for some species with missing underscore. Some inconsistency when curating the database.

# Calculate Fisher's r-to-z transformed correlation coefficient (ZCOR) as yi = effect size and vi = sampling variances, where ri = raw correlation coefficients, and ni = sample size.
zr_data <- metafor::escalc(measure = "ZCOR", ri = corr_coeff, ni = sample_size, data = zr_data, var.names = c("Zr", "V_Zr"))

# Load additional packages for phylogenetic reconstruction
pacman::p_load(rotl, ape)

# Create species list based on OTL names and match with OTL database
species <- sort(unique(as.character(zr_data$species_OTL)))  # generate list of species (as character format)
taxa <- rotl::tnrs_match_names(names = species)  # match taxonomic names to the OTL

# Check if species list do not match OTL identifier
taxa[taxa$approximate_match == TRUE, ]  # none so far

# Retrieving phylogenetic relationships among taxa in the form of a trimmed
# sub-tree
tree <- rotl::tol_induced_subtree(ott_ids = ott_id(taxa), label_format = "name")

tree_tip_label <- tree$tip.label  # extract tree tip names
species_list <- unique(stringr::str_replace_all(zr_data$species_OTL, " ", "_"))  # extract species name and add underscore to match tree_trip_label

setdiff(tree_tip_label, species_list)  # check what names from the raw data is not found in tree. 

# If names do not match the tree, then replace name in raw data to match tree

# You can also visually inspect the tree via plot function to check if the
# species extracted are correct. plot(tree, cex = 0.6, label.offset = 0.1,
# no.margin = TRUE) # plot tree

set.seed(1)
tree <- ape::compute.brlen(tree, method = "Grafen", power = 0.5)  # compute branch lengths
tree <- ape::multi2di(tree, random = TRUE)  # use a randomization approach to deal with polytomies

# Note, an internal node of a phylogenetic tree is described as a polytomy if
# (i) it is in a rooted tree and is linked to three or more subtrees or (ii) it
# is in an unrooted tree and is attached to four or more branches which is
# commonly observed with data extracted from OTL. This is the result of
# insufficient phylogenetic information such such as divergence time. To
# resolve polytomies, we use the multi2di function from the ape package.

# Check tree is ultrametric ape::is.ultrametric(tree) # TRUE

# Create correlation matrix for analysis
phylo_cor <- vcv(tree, cor = T)
```

## Phylegeny2

```{r}
par(mar=c(0,0,0,0))
plot(tree, type = "fan", adj = 0.5, cex = 0.65)
```

## Phylegeny3

```{r, phylomod, echo = TRUE}
overall_model <- metafor::rma.mv(yi = Zr, V = V_Zr, mod = ~1, random = list(~1 |
    study_ID, ~1 | es_ID, ~1 | species, ~1 | species_OTL), R = list(species_OTL = phylo_cor),
    method = "REML", test = "t", dfs = "contain", data = zr_data %>%
        dplyr::mutate(species_OTL = stringr::str_replace_all(species_OTL, " ", "_")))
summary(overall_model)
```

## Shared Control

```{r, sc1, echo = TRUE}
# Load packages

# install.packages('devtools') # We need this package to download from GitHub.
# Un-comment if you don't already have it installed.
devtools::install_github("daniel1noble/metaAidR")  # We've created a useful function for creating matrices here
pacman::p_load(metaAidR, metafor, corrplot)

# Load the data
mass_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/Mass.data.csv")
```

## Shared Control2

```{r, sc2, echo = TRUE}
## Calculate the log response ratio
mass_data <- escalc(measure = "ROM", n1i = treat_N, n2i = control_N, m1i = treat_mean,
    m2i = control_mean, sd1i = treat_error, sd2i = control_error, data = mass_data)
mass_data$obs <- 1:dim(mass_data)[1]
```

## Shared Control2

```{r, sc3, echo = TRUE}
## Calculate the M matrix
M <- make_VCV_matrix(data = mass_data, V = "vi", cluster = "comparison_ID", m = "control_mean",
    sd = "control_error", n = "control_N", type = "vcv", vcal = "ROM", obs = "obs")
```

## shared2

```{r, sc4, echo = TRUE}
## Plot the M matrix as a correlation matrix
corrplot::corrplot(cov2cor(M), tl.cex = 0.6, cl.cex = 0.6)
```

## Share4

```{r, sc5, echo = TRUE}
# full model including all random effects and moderators to do AICc model
# selection on
mlma <- rma.mv(yi = yi, V = M, mod = ~1, random = list(~1 | paper_no, ~1 | Genus_species,
    ~1 | data_ID), data = mass_data, method = "REML", test = "t", dfs = "contain")
summary(mlma)
```

## Het 1

```{r}
# install.packages("pacman") ; uncomment this line if you haven't already installed 'pacman'
pacman::p_load(metafor, tidyverse)

asr_dat <- read.csv("https://osf.io/qn2af/download")

#' @title arr
#' @description Calculates the acclimation response ratio (ARR).  
#' @param t2_l  Lowest of the two treatment temperatures
#' @param t1_h  Highest of the two treatment temperatures
#' @param x1_h  Mean trait value at high temperature
#' @param x2_l  Mean trait value at low temperature
#' @param sd1_h Standard deviation of mean trait value at high temperature
#' @param sd2_l Standard deviation of mean trait value at low temperature
#' @param n1_h  Sample size at high temperature
#' @param n2_l  Sample size at low temperature

arr <- function(x1_h, x2_l, sd1_h, sd2_l, n1_h, n2_l, t1_h, t2_l){
        ARR <- (x1_h - x2_l)/(t1_h - t2_l)
      V_ARR <- ((1/(t1_h - t2_l))^2*(sd2_l^2/n2_l + sd1_h^2/n1_h))
return(data.frame(ARR, V_ARR))
}

# Calculate the effect sizes
asr_dat<-asr_dat %>% 
              mutate(ARR= arr(x1_h = mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                              sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,1], 
                     V_ARR = arr(x1_h =  mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                           sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,2]) %>% 
                filter(sex == "female")

# Re-fit the multilevel meta-analytic model
MLMA <- metafor::rma.mv(yi= ARR~ 1, V = V_ARR, 
                   method="REML",
                   random=list(~1|species_ID,
                               ~1|study_ID,
                               ~1|es_ID), 
                   dfs = "contain",
                   test="t",
                   data=asr_dat)
```

## Het2

$$ 
\begin{equation} 
I^2_{total} = \frac{\sigma^2_{study} + \sigma^2_{phylogeny} + \sigma^2_{species} + \sigma^2_{residual}}{\sigma^2_{study} + \sigma^2_{phylogeny} + \sigma^2_{species} + \sigma^2_{residual} +\sigma^2_{m}} \
(\#eq:itot)
\end{equation} 
$$

```{r i2, echo=TRUE}
# The orchaRd package has some convenient functions for calculating various I2 estimates including total. We'll load and install that package
#install.packages("pacman")
pacman::p_load(devtools, tidyverse, metafor, patchwork, R.rsp, emmeans, flextable)

#devtools::install_github("daniel1noble/orchaRd", force = TRUE, build_vignettes = TRUE)
library(orchaRd)

orchaRd::i2_ml(MLMA, data = asr_dat)

```

## PIs

$$ 
\begin{equation} 
PI \sim  \bar{u} \pm 1.96 \sqrt{SE^2 + \sigma^2_{study} +  \sigma^2_{species} + \sigma^2_{residual}}
(\#eq:pi)
\end{equation} 
$$

```{r pis, echo=TRUE}
# Calculate the prediction intervals
predict(MLMA)
```

## 2

```{r, echo=TRUE}

l.pi.z = 0.1668 - 1.96*sqrt(MLMA$se^2 + sum(MLMA$sigma2))
u.pi.z = 0.1668 + 1.96*sqrt(MLMA$se^2 + sum(MLMA$sigma2))

c(l.pi.z, u.pi.z)

l.pi.t = 0.1668 - qt(0.975, df = 20)*sqrt(MLMA$se^2 + sum(MLMA$sigma2))
u.pi.t = 0.1668 + qt(0.975, df = 20)*sqrt(MLMA$se^2 + sum(MLMA$sigma2))
c(l.pi.t, u.pi.t)
```

## MetaReg

```{r metareg, echo = TRUE}
meta_reg <- metafor::rma.mv(yi= ARR ~ 1 + method + class, V = V_ARR, method="REML", 
                            random=list(~1|species_ID, 
                                        ~1|study_ID, 
                                        ~1|es_ID), 
                            dfs = "contain", test="t", data=asr_dat)
meta_reg
```

## Orchard

```{r orchard, echo = TRUE}

orchaRd::orchard_plot(meta_reg, data = asr_dat, mod = "class", group = "study_ID", xlab = "Acclimation Response Ratio (ARR)", angle = 45)
```

## Orchard2

```{r orchard2, echo = TRUE}
mod_table <- orchaRd::mod_results(meta_reg,  data = asr_dat,  mod = "class", group = "study_ID", at = list(class = c("Malacostraca", "Insecta", "Actinopterygii", "Amphibia")), subset = TRUE)

orchaRd::orchard_plot(mod_table, xlab = "Acclimation Response Ratio (ARR)", angle = 45)

```

## Pub bias

```{r}
# Packages
pacman::p_load(tidyverse, metafor, orchaRd)

# Download the data. Exclude NA in r and sample size columns
arnold_data <- read.csv("https://raw.githubusercontent.com/pieterarnold/fitness-rmr-meta/main/MR_Fitness_Data_revised.csv")

# Exclude some NA's in sample size and r
arnold_data <- arnold_data[complete.cases(arnold_data$n.rep) & complete.cases(arnold_data$r),
    ]

# Calculate the effect size, ZCOR
arnold_data <- metafor::escalc(measure = "ZCOR", ri = r, ni = n.rep, data = arnold_data,
    var.names = c("Zr", "Zr_v"))

# Lets subset to endotherms for demonstration purposes
arnold_data_endo <- arnold_data %>%
    mutate(endos = ifelse(Class %in% c("Mammalia", "Aves"), "endo", "ecto")) %>%
    filter(endos == "endo" & Zr <= 3)  # Note that one sample that was an extreme outlier was removed in the paper.

# Add in observation-level (residual)
arnold_data_endo$obs <- 1:dim(arnold_data_endo)[1]
```

## Pub bias 2

```{r, funnel, echo=TRUE}
par(mfrow=c(1,1))
metafor::funnel(x = arnold_data_endo$Zr, vi = arnold_data_endo$Zr_v, yaxis = "seinv",
    digits = 2, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = tanh, legend = TRUE)
```

## Pub bias 3

```{r}
# Fitness type
metareg <- rma.mv(yi = Zr, V = Zr_v, mods = ~FitnessClassification, data = arnold_data_endo,
    random = list(~1 | Ref, ~1 | obs))


# Extract residuals based on the fixed effect predictions (marginal) and the
# fixed and random effect (conditional)
resid_marg <- arnold_data_endo$Zr - predict(metareg)[[1]]  # Calculates the residuals based on fixed effects only
vi_residm <- arnold_data_endo$Zr_v + predict(metareg)[[2]]^2  # Adds prediction errors

# Conditional requires random effect blups for study (Ref) and effect size obs
blups <- ranef(metareg)  # extracts blups for each random effect. Stored as a list.

study_ef <- blups$Ref[[1]][match(arnold_data_endo$Ref, row.names(blups$Ref))]  # This code will expand each random effect for study out to match the study ID across the entire dataset
study_vi <- (blups$Ref[[2]][match(arnold_data_endo$Ref, row.names(blups$Ref))])^2

effect_effect <- blups$obs[[1]]
vi_effect <- blups$obs[[1]]^2

# Now we have everything we need to calculate residuals
resid_cond <- arnold_data_endo$Zr - (predict(metareg)[[1]] + study_ef + effect_effect)
vi_resid_cond <- vi_residm + study_vi + vi_effect

# Lets make a funnel plot to visualize the data in relation to the precision,
# inverse sampling standard error,
par(mfrow = c(1, 2))
metafor::funnel(x = resid_marg, vi = vi_residm, yaxis = "seinv", digits = 2, level = c(0.1,
    0.05, 0.01), shade = c("white", "gray55", "gray 75"), las = 1, xlab = "Meta-analytic Residuals (marginal)",
    legend = FALSE)
metafor::funnel(x = resid_cond, vi = vi_resid_cond, yaxis = "seinv", digits = 2,
    level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"), las = 1,
    xlab = "Meta-analytic Residuals (conditional)", legend = TRUE)
```

## Pub bias 4

$$
y_{i} = \mu + \beta_{se}se +  s_{j[i]} + spp_{k[i]} + e_{i} + m_{i}
$$

## Pub bias 4.3

$$
y_{i} = \mu + \sum_{i = 1}^{N_{m}}\beta_{m}x_{m} + \beta_{se}se + \beta_{y}Year +  s_{j[i]} + spp_{k[i]} + e_{i} + m_{i}
$$

## Pub bias 5

```{r, echo = TRUE}
# Including sampling standard error as moderator
metareg_se <- rma.mv(yi = Zr, V = Zr_v, mods = ~ sqrt(Zr_v), test = "t", dfs = "contain", data = arnold_data_endo, random = list(~1|Ref, ~1|obs))
summary(metareg_se)
```

## Pub bias 6

```{r, echo = TRUE}
# Including sampling standard error as moderator and year
arnold_data_endo$se <- sqrt(arnold_data_endo$Zr_v)
arnold_data_endo$c_log_year <- scale(log(arnold_data_endo$Year))

# Fit model
metareg_se_time <- rma.mv(yi = Zr, V = Zr_v, mods = ~ se + c_log_year, test = "t", dfs = "contain", data = arnold_data_endo, random = list(~1|Ref, ~1|obs))
summary(metareg_se_time)

# Bubble plots!
p1 <- orchaRd::bubble_plot(metareg_se_time, mod = "se", group = "Ref", data = arnold_data_endo, xlab = "Sampling Standard Error (SE)", ylab = "Zr", legend.pos = "bottom.left")

p2 <- orchaRd::bubble_plot(metareg_se_time, mod = "c_log_year", group = "Ref", data = arnold_data_endo, xlab = "Publication Year (log-transformed & scaled)", ylab = "Zr", legend.pos = "none")

p1 | p2



```
