---
title: "Meta-analysis in Comparative Physiology: A brief introduction to effect sizes and meta-analytic modelling"
author: Daniel W.A Noble, Nicholis Wu, Essie Rodgers, Patrice Pottier
output: powerpoint_presentation
date: '`r Sys.Date()`'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr, magick)
```

## Slide with Bullets {.secondslide}

- Bullet 1
- Bullet 2
- Bullet 3

$$
y_{i} = x^2
$$

> - Eat eggs
> - Drink coffee

## Slide with R Output {.thirdslide}

```{r cars, echo = TRUE, tidy=TRUE}
# LDH activity in 8 C treatment
 trt_8C <- rnorm(10, 0.40, 0.5)

# LDH activity in 26 C treatment
trt_26C <- rnorm(10, 0.60, 0.20)

# What's the difference, i.e., effect size, between treatments?
mean(trt_26C - trt_8C)
t.test(trt_26C, trt_8C)
```


## Do 1000s of experiments

```{r sim, echo = TRUE, tidy=TRUE}

experiment <- function(times){
  
  dat <- data.frame(matrix(nrow = times, ncol = 2))
  
  for(i in 1:times){
  # LDH activity in 8 C treatment
   trt_8C <- rnorm(10, 0.40, 0.5)
  
  # LDH activity in 26 C treatment
  trt_26C <- rnorm(10, 0.60, 0.20)
  
  # What's the difference, i.e., effect size, between treatments?
  dat[i,1] <- mean(trt_26C - trt_8C)
  dat[i,2] <- t.test(trt_26C, trt_8C)$p.value
    
  }
  names(dat) <- c("effect", "p")
  return(dat)
}

expts <- experiment(10000)
```

## Figure

```{r simfig, echo = FALSE, tidy=TRUE}
par(mfrow = c(1,2))
hist(expts$effect, xlab = "Effect size", main = "")
hist(expts$p, xlab = "p-value", main = "")
```

## errors

$$
SE = SD / \sqrt{N}
$$


```{r simfig2, echo = TRUE, tidy=TRUE}
sd(expts$effect)

```



## se

```{r simfig3, echo = TRUE, eval = FALSE, tidy=TRUE}
  means <- c()

   for (i in 1:1000000){
      mass <- rnorm(n = 18, mean = 10.8, sd = 0.988)
      means <- c(means, mean(mass))
    }

   # The mean of the means
   mean(means)
   # The standard deviation of the means or standard error
   sd(means)
```

## Our analytical calculaton

```{r se, echo = TRUE, eval = FALSE, tidy=TRUE}
se <- sd(mass) / sqrt(length(mass))
se
```
## Plots
```{r simfig3plot, echo = TRUE, eval = FALSE, tidy=TRUE}
   ggplot() + geom_histogram(aes(x = means))
```


## Zr

```{r Zr,  echo = TRUE}
zr_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/ind_disp_raw_data.csv") %>%
  dplyr::select(study_ID, taxa, species, trait, response, response_unit, disp_trait, disp_unit, corr_coeff, sample_size) %>% # remove irrelevant columns for this tutorial
  dplyr::top_n(10) # select first 10 effect sizes to illustrate escalc function
```

## Effect calculation
```{r Zrescalc,  echo = TRUE}
# Calculate Fisher's r-to-z transformed correlation coefficient (ZCOR) as yi = effect size and vi = sampling variances, where ri = raw correlation coefficients, and ni = sample size.
zr_data <- metafor::escalc(measure = "ZCOR", ri = corr_coeff, ni = sample_size, data = zr_data, var.names=c("Zr","v_Zr"))

zr_data

metafor::forest(x = zr_data$Zr[1:6], vi = zr_data$v_Zr[1:6], xlab = "Z-transformed Correlation (Zr)")
```

## Back transofrmation

```{r Zrescalcbackcalc, echo = TRUE}
# We can easily convert back to r as follows
zr_data$r <- tanh(zr_data$Zr)

zr_data %>% select(corr_coeff, r)
```


## Hedges

```{r lnRR, echo = TRUE}
contrast_data <- read.csv("https://raw.githubusercontent.com/daniel1noble/meta-workshop/gh-pages/data/pop_disp_raw_data.csv") %>%
  dplyr::select(study_ID, taxa, species, trait, response, response_unit, mean_core, sd_core, n_core, mean_front, sd_front, n_front) %>% # remove irrelevant columns for this tutorial
  dplyr::top_n(10) # select first 10 effect sizes to illustrate escalc function
```

## Hedgesg
```{r hedges, echo = TRUE}
# Calculate Hedges' g as g = effect size and v_g = sampling variances, where m1i = mean of edge population, n1i = sample size of edge population, sd1i = standard deviation of edge population, m2i = mean of core population, n2i = sample size of core population, sd2i = standard deviation of core population.
contrast_data <- metafor::escalc(measure = "SMD", 
                              m1i = mean_front, n1i = n_front, sd1i = sd_front,
                              m2i = mean_core, n2i = n_core, sd2i = sd_core, 
                              data = contrast_data, var.names=c("g","v_g"))
contrast_data %>% select(study_ID, mean_core, sd_core, n_core, mean_front, sd_front, n_front, g, v_g)
```


## lnRR
```{r lnRREffects, echo = TRUE, eval = FALSE}
# Calculate log response ratio, lnRR = effect size and v_lnRR = sampling variances, where m1i = mean of edge population, n1i = sample size of edge population, sd1i = standard deviation of edge population, m2i = mean of core population, n2i = sample size of core population, sd2i = standard deviation of core population.
contrast_data <- metafor::escalc(measure = "ROM", 
                              m1i = mean_front, n1i = n_front, sd1i = sd_front,
                              m2i = mean_core, n2i = n_core, sd2i = sd_core, 
                              data = contrast_data, var.names=c("lnRR","v_lnRR"))
contrast_data %>% select(study_ID, mean_core, sd_core, n_core, mean_front, sd_front, n_front, lnRR, v_lnRR)
#contrast_data$cluster <- c(1,1,1,2,2,3,4,5,6,7,8)
contrast_data$obs <- 1:nrow(contrast_data)
data <- contrast_data
covariance_funtions <- function(data, m, sd, n, cov_type){

  if(cov_type == "ROM"){
    # Covariance for shared control when using log response ratio. From Jageunesse 2011. Ecology
    cov <- data[, sd]^2 / (data[, n] * data[, m]^2)
  }

  if(cov_type == "LOR"){
    #formula for odds ratio (1/x + 1/(m-x))
    cov <- (1/data[, m] + 1) / (data[, n] - data[, m])
  }
  return(cov)
}

# Step 1: split on cluster
splt <- split(data, data$cluster)

# Step 2: calculate the covariance. Because cluster will have repeated values these will be the same within a study
data$cov <- unlist(lapply(splt, function(x) 
                  if(dim(x)[1] > 1) {
                    covariance_funtions(x, m = "mean_core",sd = "sd_core", n = "n_core", cov_type = "ROM") } else { 0}))

# Step 3: build matrix
m <- diag(data$v_lnRR)

m <- lapply(splt, function(x) matrix(x$v_lnRR))

m_splt <- split(m, data$cluster)
bdiag(m)
m[which(duplicated(data[,cluster])==TRUE), which(duplicated(data[,cluster])==TRUE)] <- data$cov[which(duplicated(data[,cluster])==TRUE)]

  m[upper.tri(m)] <- t(m[lower.tri(m)])

mat_splt <- lapply(splt, function(x) diag(x$v_lnRR))



mat_splt[[2]][lower.tri(mat_splt[[2]])] <- data$cov[1:3]


### Testing out new make_VCV with contrast_data as example
contrast_data$cluster <- c(1,1,2,2,3,3,3,0,0,0,0)
contrast_data$cluster <- paste0(contrast_data$cluster,".", contrast_data$study_ID)

metaAidR::make_VCV_matrix(data = contrast_data, V = "v_lnRR", obs = "obs", cluster = "cluster", type = "cor")

make_VCV_matrix(contrast_data, V = "v_lnRR", obs = "obs", cluster = "cluster", m = "mean_core", sd = "sd_core", n = "n_core", cov_type = "ROM")
```

## COnverting back to understand lnRR

```{r ConvertLnRR, echo = TRUE}
### Let's back calculate effects to make sure we understand why they are interpreted as percentage differences

# Make sure we understand how it's calculated
with(contrast_data, log(mean_front[1] / mean_core[1]))

# Alternatively....
with(contrast_data, log(mean_front[1]) - log(mean_core[1]))

# Now lets back-transform to odds. 
with(contrast_data, exp(log(mean_front[1]) - log(mean_core[1])))

# Interpretation: What this tells us is that the numerator is 1.15 times the denominator, or, that the marginal (front) mean is 15% larger compared to the core mean. We can see that this is true as follows:
with(contrast_data, exp(log(mean_front[1]) - log(mean_core[1])) * mean_core[1])

# This value should now match the marginal mean value, which it does
with(contrast_data, mean_front[1])
```


## Latency

$$
ln \bar{X} = log(\bar{X}) - log \sqrt{ \left ( 1 + \frac{SD^2}{\bar{X}^2}\right )}
$$

$$
lnSD = \sqrt{ log \left (   1 + \frac{SD^2}{\bar{X}^2} \right )}
$$

## Proportions

$$
\bar{X_{p}} = log \left ( \frac{p}{ 1 - p } \right ) 
$$

$$
SD_{p} = \sqrt{SD^2 \left ( \frac{1}{p} \right ) + \left ( \frac{p}{ 1 - p^2} \right )}
$$

## Arcsine transformation
$$
\bar{X_{t}} = arcsine(\sqrt{\bar{{X}}})
$$
$$
SD_{t} = \sqrt{\frac{SD^2}{4\bar{X}\left ( 1- \bar{X}\right ) }}
$$
## Geary test - Counts

$$
\frac{\bar{X}}{SD} \left ( \frac{4N^{3/2}}{1 + 4N} \right ) \geq 3
$$

## Q10

$$
\begin{equation} 
  lnRR_{Q_{10}} = ln\left( \frac{R_{2}}{R_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }
(\#eq:lnq10)
\end{equation} 
$$ 

$$
\begin{equation} 
  s^2_{lnRR_{Q_{10}}} = \left( \frac{SD_{2}^2}{R^2_{2}N_{2}} + \frac{SD_{1}^2}{R^2_{1}N_{1}} \right){ \left(\ \frac{10^{\circ}C}{T_{2}-T_{1}} \right) }^2
  (\#eq:Vlnq10)
\end{equation} 
$$


## Fixed effect


```{r fixmodel, echo = TRUE}

# Generate some simulated effect size data with known sampling variance
# assumed to come from a common underlying distribution
set.seed(86) # Set see so that we all get the same simulated results
 # We will have 5 studies
    stdy  <- 1:5                                      
# We know the variance for each effect
    Ves     <- c(0.05, 0.10, 0.02, 0.10, 0.09)    
# We'll need this later but these are weights
    W     <- 1 / Ves                                          
# We assume they are sampled from a normal distribution  with a mean effect size of 2
    es     <- rnorm(length(Ves), 2, sqrt(Ves))          
# Data for our fixed effect meta-analysis
    dataFE <- data.frame(stdy = stdy, es, Ves)

```


## Fixed effects
```{r fePLOT, echo = FALSE}
  
  plot(stdy - 0.25 ~ es, col = "black", pch = 16, ylim = c(0, 6), xlim = c(1, 3.2), 
       data = dataFE, ylab = "Study", xlab = "Effect size")
  abline(v = 2, lty =3)
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es + sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es - sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
```


## Fixed effect model


```{r, fixedma,  eval=TRUE, echo=TRUE}
# Run a fixed effect meta-analysis using the FE dataset. 
    metafor::rma(yi = es, vi = Ves, method = "FE", data = dataFE)
```

## Now by hand FE

We can even do all the seemingly fancy stuff `metafor` is doing ourselves if we want....we just need to know the equations:
 $$
  \bar{ES}= \sum{\left (W*ES \right )}/ \sum{W}
 $$
$$
  \sigma_{\bar{ES}}^2= \frac{1}{\sum{W}}
 $$
 
```{r, fixedmahand,  eval=TRUE, echo=TRUE}
# Calculate pooled effect size
       EsP.FE <- sum(W*dataFE$es) / sum(W)
       EsP.FE
# Calculate the pooled variance around estimate
    VarEsP.FE <- 1 / sum(W)
    VarEsP.FE
# Calculate the standard error around estimate
    SE.EsP.FE <- sqrt(VarEsP.FE)
    SE.EsP.FE
```

## Random effect in code

```{r randomSim , echo = TRUE}
# Here adding 0.8 says we want to add 0.8 as the between study variability. In other words, each effect size is sampled from a larger distribution of effect sizes that itself comes from a distribution with a variance of 0.8. 
    esRE        <- rnorm(length(Ves), 2, sqrt(Ves + 0.8)) 
# Data for our random effect meta-analysis 
    dataRE <-  data.frame(stdy = stdy, esRE, Ves)
```

## Random effect plot

```{r, fvsr, eval=TRUE, echo=FALSE, fig.cap="Mean (arrows are sampling standard deviation) effect size for each study. Data simulated under a fixed effect model in black and data simulated under a random effect model in red."}
    
  plot(stdy - 0.25 ~ es, col = "black", pch = 16, ylim = c(0, 6), xlim = c(1, 3.2), 
       data = dataFE, ylab = "Study", xlab = "Effect size")
  abline(v = 2, lty =3)
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es + sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es - sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  
  points(stdy + 0.25 ~ esRE, col = "red", pch = 16, data = dataRE)
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es + sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es - sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  
```
## random effect model


```{r, re,  eval=TRUE, echo=TRUE}
metafor::rma(yi = esRE, vi = Ves, method="DL", data = dataRE)
```

## Random by hand
We first need to estimate $\tau^2$ or the between-study variance which can be calculated from these equations:

$$\tau^2 = \frac{Q - df}{C} $$
where 

$$ Q = \sum_{i=1}^{k}{W_{i}ES_{i}^2} - \frac{ \left (\sum_{i=1}^{k}{W_{i}ES_{i}} \right)^2}{\sum_{i=1}^{k}{W_{i}}}$$
$$ C = \sum{W_{i}} - \frac{\sum{W_{i}^2}}{\sum{W_{i}}}$$

## Random effect by hand 2
```{r, re_hand, eval=TRUE, echo=TRUE}
# Calculate our Q statistic again
    Q <- sum(W*(dataRE$es^2) ) - (sum(W*dataRE$es)^2 / sum(W))
    Q

# Calculate tau2
    C <- sum(W) - ((sum(W^2))/sum(W))
    C

# Calculate df
    df <- nrow(dataRE) - 1   
    T2 <- (Q - df) / C
    T2
```


## Now that we have tau2 lets do the meta-analysis
Remember, things are the same but the weighting is different now.

```{r, re_w, eval=TRUE, echo=TRUE}
    # RE weights
      W.re <- 1 / (T2 + dataRE$Ves)
    
    #Pooled effect size for random effect meta-analysis
      esPoolRE  <- sum(W.re*dataRE$es) / sum(W.re) 
      esPoolRE
    
    # Calculate the pooled variance around estimate
       VarES <- 1 / sum(W.re)
    
    # Calculate the standard error around estimate
       SE.ES.RE <- sqrt(VarES)
       SE.ES.RE
```

## Q10 write our functioon

```{r ln10funcs, echo = TRUE, message=FALSE, warning=FALSE}

#' @title lnRR_Q10
#' @description Calculates the log Q10 response ratio.  Note that temperature 2 is placed in the numerator and temperature 1 is in the denominator
#' @param t1  Lowest of the two treatment temperatures
#' @param t2  Highest of the two treatment temperatures
#' @param r1  Mean physiological rate for temperature 1
#' @param r2  Mean physiological rate for temperature 2
#' @param sd1 Standard deviation for physiological rates at temperature 1
#' @param sd2 Standard deviation for physiological rates at temperature 2
#' @param n1  Sample size at temperature 1
#' @param n2  Sample size at temperature 2
#' @param name Character string for column name
#' @example
#' lnRR_Q10(20, 30, 10, 5, 1, 1, 30, 30)
#' lnRR_Q10(20, 30, 10, 5, 1, 1, 30, 30, name = "acclim")
#' @export

lnRR_Q10 <- function(t1, t2, r1, r2, sd1, sd2, n1, n2, name ="acute"){
        
    lnRR_Q10 <- (10 / (t2 - t1))   * log(r2 / r1)
  V_lnRR_Q10 <- (10 / (t2 - t1))^2 * ((sd1^2 / (n1*r1^2)) + (sd2^2 / (n2*r2^2)))
  
              dat <- data.frame(lnRR_Q10, V_lnRR_Q10)
    colnames(dat) <- c(paste0("lnRR_Q10", name),  
                       paste0("V_lnRR_Q10", name))
  return(dat)
}

```

## Q10
```{r ln101, echo = TRUE, message=FALSE, warning=FALSE}
q10_dat <- read.csv("https://osf.io/download/fb3ht/") 

# Calculate lnRRQ10 and it's associated sampling variance
q10_dat <- cbind(q10_dat, with(q10_dat, lnRR_Q10(t1 = t1, t2 = t2, r1 = mean_t1, r2 = mean_t2, sd1 = sd_t1, sd2 = sd_t2, n1 = n_t1, n2 = n_t2, name = "")))
head(q10_dat)

```

## Q10 Interpretation
```{r ln102, echo = TRUE,message=FALSE, warning=FALSE}
# Now we can back-calculate to put on the original Q10 scale. We can than check that this matches the Q10 already in the data
head(q10_dat %>% mutate(exp_lnRR_Q10 = exp(lnRR_Q10)) %>% select(exp_lnRR_Q10, Q10))

```

**For the first row of our data we can see that dive duration when temperatures increase by 10$^{\circ}$C is expected to decrease by ~67% (1-0.33)**.


## MLMA

```{r, echo = TRUE}
# install.packages("pacman") ; uncomment this line if you haven't already installed 'pacman'
pacman::p_load(metafor, tidyverse)

asr_dat <- read.csv("https://osf.io/qn2af/download")
```

We'll also need our function for calculating ARR and its sampling variance because these don't exist in any current packages. 

```{r ARRfunc, echo = TRUE}
#' @title arr
#' @description Calculates the acclimation response ratio (ARR).  
#' @param t2_l  Lowest of the two treatment temperatures
#' @param t1_h  Highest of the two treatment temperatures
#' @param x1_h  Mean trait value at high temperature
#' @param x2_l  Mean trait value at low temperature
#' @param sd1_h Standard deviation of mean trait value at high temperature
#' @param sd2_l Standard deviation of mean trait value at low temperature
#' @param n1_h  Sample size at high temperature
#' @param n2_l  Sample size at low temperature

arr <- function(x1_h, x2_l, sd1_h, sd2_l, n1_h, n2_l, t1_h, t2_l){
        ARR <- (x1_h - x2_l)/(t1_h - t2_l)
      V_ARR <- ((1/(t1_h - t2_l))^2*(sd2_l^2/n2_l + sd1_h^2/n1_h))
return(data.frame(ARR, V_ARR))
}
```

## Caluclate ARR
```{r, tidy=TRUE, echo = TRUE}
# Calculate the effect sizes
asr_dat<-asr_dat %>% 
              mutate(ARR= arr(x1_h = mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                              sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,1], 
                     V_ARR = arr(x1_h =  mean_high, x2_l = mean_low, t1_h = acc_temp_high, t2_l = acc_temp_low, 
                           sd1_h = sd_high, sd2_l = sd_low, n1_h = n_high_adj, n2_l = n_low_adj)[,2]) %>% 
                filter(sex == "female")
```

## Fit model

```{r, class.source='klippy', echo = TRUE}
# Multi-level meta-analytic model
MLMA <- metafor::rma.mv(yi= ARR~ 1, V = V_ARR, 
                   method="REML",
                   random=list(~1|species_ID,
                               ~1|authors,
                               ~1|es_ID), 
                   dfs = "contain",
                   test="t",
                   data=asr_dat)
print(MLMA)

```
