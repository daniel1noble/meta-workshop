---
title: "Fixed versus Random Effects Meta-analysis"
author: Daniel Noble
date: '`r Sys.Date()`'
bibliography: ./bib/refs.bib
csl: ./bib/the-journal-of-experimental-biology.csl
output:
  bookdown::html_document2:
    css: style.css
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, tidy = TRUE)
options(digits=2)
```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
#install.packages("devtools")
remotes::install_github("rlesur/klippy")
klippy::klippy(tooltip_message = 'Click to Copy Code', tooltip_success = 'Done', position = 'right', color = "red")

# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr)

# To use mathjaxr 
```

## What's the difference between fixed and random effects meta-analysis?

To demonstrate, we can simulate some data. This is also quite useful for getting a handle on what the difference between fixed and random effects meta-analytic models looks like. Either way, we'll explore the data in depth as we move through the tutorial. 

```{r, simdat, eval=TRUE, echo=TRUE}
# Generate some simulated effect size data with known sampling variance
# assumed to come from a common underlying distribution
set.seed(86) # Set see so that we all get the same simulated results
 # We will have 5 studies
    stdy  <- 1:5                                      
# We know the variance for each effect
    Ves     <- c(0.05, 0.10, 0.02, 0.10, 0.09)    
# We'll need this later but these are weights
    W     <- 1 / Ves                                          
# We assume they are sampled from a normal distribution  with a mean effect size of 2
    es     <- rnorm(length(Ves), 2, sqrt(Ves))          
# Data for our fixed effect meta-analysis
    dataFE <- data.frame(stdy = stdy, es, Ves)

# Generate a second set of effect sizes, but now assume that each study effect does not come from the same distribution, but from a population of effect sizes. 

# Here adding 0.8 says we want to add 0.8 as the between study variability. In other words, each effect size is sampled from a larger distribution of effect sizes that itself comes from a distribution with a variance of 0.8. 
    esRE        <- rnorm(length(Ves), 2, sqrt(Ves + 0.8)) 
# Data for our random effect meta-analysis 
    dataRE <-  data.frame(stdy = stdy, esRE, Ves)
```

We can get a look at what these two datasets we simulated above look like (Figure \@ref(fig:fvsr)). The red circles are effect sizes and their standard errors (square root of the sampling error variance) from our fixed effect meta-analysis (FE) data set. In contrast, the black circles are our effect size and standard errors from the data generated in the random effect meta-analysis (RE) dataset. The black line is the average, true effect size (which we have set to the value 2).

```{r, fvsr, eval=TRUE, echo=FALSE, fig.cap="Mean (arrows are sampling standard deviation) effect size for each study. Data simulated under a fixed effect model in black and data simulated under a random effect model in red."}
    
  plot(stdy - 0.25 ~ es, col = "black", pch = 16, ylim = c(0, 6), xlim = c(1, 3.2), 
       data = dataFE, ylab = "Study", xlab = "Effect size")
  abline(v = 2, lty =3)
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es + sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  arrows(x0 = dataFE$es, y0 = dataFE$stdy- 0.25, y1 = dataFE$stdy- 0.25, x1 = dataFE$es - sqrt(dataFE$Ves), 
         angle = 90, length = 0.05, col = "black")
  
  points(stdy + 0.25 ~ esRE, col = "red", pch = 16, data = dataRE)
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es + sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  arrows(x0 = dataRE$es, y0 = dataRE$stdy+ 0.25, y1 = dataRE$stdy+ 0.25, x1 = dataRE$es - sqrt(dataRE$Ves), 
         angle = 90, length = 0.05, col = "red")
  
```

We notice a few important differences here. In the RE dataset the variance across the studies is a lot larger compared to the FE dataset. This is what we would expect because we have added in a between study variance. Now, let’s use a common meta-analysis package, *metafor*, to analyse these datasets.

### Fixed effect meta-analysis with `metafor`

```{r, fixedma,  eval=TRUE, echo=TRUE}
# Run a fixed effect meta-analysis using the FE dataset. 
    metafor::rma(yi = es, vi = Ves, method = "FE", data = dataFE)
```

So this suggests that the average estimate is 2.07 and it has a standard error of 0.1. It also provides us with a Q statistic, which measures the amount of heterogeneity among effect sizes. If Q is large and the p-value is small then it suggests substantial heterogeneity among effect sizes beyond what we would expect if these effects were generated from a common underlying distribution. This is a major assumption of fixed effect meta-analyses and in this case is supported. This is a good thing because we specifically generated these data under the assumption that they are from the same distribution. We can see this if we look at how we generated the data: `rnorm(length(Ves), 2, sqrt(Ves))`. This draws random effect sizes from a normal distribution with a variance (`sqrt(Ves)`) for each effect size that is only defined by its sampling variability.

### Fixed effect meta-analysis by hand!
What is the model above doing though? and what is the logic behind the calculations? The best way to understand this is to hand calculate these values. Basically the effect sizes are being weighted by their sampling error variance when deriving the pooled estimate and it’s variance. Let’s calculate this by hand and see what’s happening:

```{r, fixedmahand,  eval=TRUE, echo=TRUE}
# Calculate pooled effect size
       EsP.FE <- sum(W*dataFE$es) / sum(W)
       EsP.FE
# Calculate the pooled variance around estimate
    VarEsP.FE <- 1 / sum(W)
    VarEsP.FE
# Calculate the standard error around estimate
    SE.EsP.FE <- sqrt(VarEsP.FE)
    SE.EsP.FE
```

Wow! This is so cool. We just did a fixed effect meta-analysis by hand…and, look, it matches the model output perfectly. The math is not so scary after all! But what about this extra statistic, Q? How do we derive this?

```{r, q_fe,  eval=TRUE, echo=TRUE}
#Now lets calculate our Q. Q is the total amount of heterogeneity in our data.
    Q.fe <- sum(W*(dataFE$es^2) ) - (sum(W*dataFE$es)^2 / sum(W))
    Q.fe
```

Cool. So this value also matches up nicely. 


### Random effect meta-analysis using `metafor`
Now lets move on to a more complicated situation, a random effect meta-analysis using the RE dataset. Remember, we know from this data set that each effect size comes from a different overall distribution. How might Q change? (Hint: We expect it to get larger!)

```{r, re,  eval=TRUE, echo=TRUE}
metafor::rma(yi = esRE, vi = Ves, method="REML", data = dataRE)
```

What happened here? What does all this mumbo jumbo actually mean?! As we predicted, our Q has jumped up a lot! Why is this so? That’s because we added in extra variability to the mix because each effect size is to come from a distribution of true effect sizes. Because they can all have their own mean they can differ substantially from the other effect sizes and this results in more variance (i.e. heterogeneity) being added to our data over all.

### Random effect meta-analysis by hand!
Now lets see if we can reproduce these results. We need to remember that we need to add in the between study variance to our weighting of effect sizes for this model. To do this, we need to estimate how much heterogeneity we have between studies above what we would expect from sampling variability. This can be estimated by calculating the $\tau^2$ statistic, which is the variance in the ‘true’ effect sizes. To calculate this we need to calculate Q using our weights, which are the same because we know these to be true.

```{r, re_hand, eval=TRUE, echo=TRUE}
# Calculate our Q statistic again
    Q <- sum(W*(dataRE$es^2) ) - (sum(W*dataRE$es)^2 / sum(W))
    Q

# Calculate tau2
    C <- sum(W) - ((sum(W^2))/sum(W))
    C

# Calculate df
    df <- nrow(dataRE) - 1   
    T2 <- (Q - df) / C
    T2
```

Now we can re-calculate our weights by adding in the between study heterogenetity.

```{r, re_w, eval=TRUE, echo=TRUE}
    # RE weights
      W.re <- 1 / (T2 + dataRE$Ves)
    
    #Pooled effect size for random effect meta-analysis
      esPoolRE  <- sum(W.re*dataRE$es) / sum(W.re) 
      esPoolRE
    
    # Calculate the pooled variance around estimate
       VarES <- 1 / sum(W.re)
    
    # Calculate the standard error around estimate
       SE.ES.RE <- sqrt(VarES)
       SE.ES.RE
```
             
### Different estimators of parameters will give different answers
OK. What happened above? Our Q statistic is correct but $\tau^2$, our mean estimate and it’s standard error are different. Why? This is because the `metafor` model we ran is using a different estimation method (REML) to estimate these statistics compared to our hand calculations. But, we are awfully close on all our estimates in any case! However, we can re-run this all using the method we used for our hand calculations above and get a nice match between our calculations and the models.

```{r, dl, eval=TRUE, echo=TRUE}
 metafor::rma(yi = esRE, vi = Ves, method="DL", data = dataRE)
```

Now we have a better understanding of the difference between fixed and random-effect meta-analysis, and, we have even been able to do our own analysis by hand! How cool is that?!
